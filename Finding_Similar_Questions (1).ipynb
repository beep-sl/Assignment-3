{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Stephen Lanna\n",
        "# Assignment 3\n",
        "# NLP\n",
        "# University of Southern Maine"
      ],
      "metadata": {
        "id": "uBQKSAU0jNvy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example of training fastText model and getting sentence embeddings"
      ],
      "metadata": {
        "id": "xl0K2GQZe8kb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import re"
      ],
      "metadata": {
        "id": "6YOiuml12GbG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fae3b83-2a73-4b6c-f7ef-d57f61522cc5"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "HCFRB215b45H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "709f0d45-0277-465a-bdae-467635594c88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.word2vec:Each 'sentences' item should be a list of words (usually unicode strings). First item here is instead plain <class 'str'>.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7853913903236389\n",
            "0.8745558857917786\n"
          ]
        }
      ],
      "source": [
        "from gensim.models import FastText\n",
        "from scipy import spatial\n",
        "\n",
        "def get_sentence_embedding(model, sentence):\n",
        "  # This method takes in the trained model and the input sentence\n",
        "  # and returns the embedding of the sentence as the average embedding\n",
        "  # of its words\n",
        "  words = sentence.split(\" \")\n",
        "  vector = model.wv[words[0]].copy()\n",
        "  for i in range(1, len(words)):\n",
        "    vector += model.wv[words[i]]\n",
        "  return vector/len(words)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 1"
      ],
      "metadata": {
        "id": "IipmHbhciLGa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reading Law Stack Exchange Data"
      ],
      "metadata": {
        "id": "1vC6-KZifEAP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "from post_parser_record import PostParserRecord\n",
        "import re\n",
        "\n",
        "def read_tsv_test_data(file_path):\n",
        "  # Takes in the file path for test file and generate a dictionary\n",
        "  # of question id as the key and the list of question ids similar to it\n",
        "  # as value. It also returns the list of all question ids that have\n",
        "  # at least one similar question\n",
        "  dic_similar_questions = {}\n",
        "  lst_all_test = []\n",
        "  with open(file_path) as fd:\n",
        "    rd = csv.reader(fd, delimiter=\"\\t\", quotechar='\"')\n",
        "    for row in rd:\n",
        "        question_id = int(row[0])\n",
        "        lst_similar = list(map(int, row[1:]))\n",
        "        dic_similar_questions[question_id] = lst_similar\n",
        "        lst_all_test.append(question_id)\n",
        "        lst_all_test.extend(lst_similar)\n",
        "  return dic_similar_questions, lst_all_test\n",
        "\n",
        "\n",
        "def train_model(lst_sentences):\n",
        "  model = FastText(sg=1,vector_size=100,window=5,min_n=1)\n",
        "  model.build_vocab(lst_sentences)\n",
        "  model.train(lst_sentences, total_examples=len(lst_sentences), epochs=10)\n",
        "  model.save(\"my_model.model\")\n",
        "\n",
        "  return model\n",
        "\n",
        "\n",
        "\n",
        "duplicate_file = \"duplicate_questions.tsv\"\n",
        "post_file = \"Posts_law.xml\"\n",
        "dic_similar_questions, lst_all_test = read_tsv_test_data(duplicate_file)\n",
        "post_reader = PostParserRecord(post_file)\n",
        "\n",
        "lst_training_sentences = []\n",
        "for question_id in post_reader.map_questions:\n",
        "  if question_id in lst_all_test:\n",
        "    continue\n",
        "  question = post_reader.map_questions[question_id]\n",
        "  title = question.title\n",
        "  body = question.body\n",
        "  # Collect sentences here\n",
        "  title_sentences = nltk.sent_tokenize(question.title)\n",
        "  body_sentences = nltk.sent_tokenize(question.body)\n",
        "\n",
        "  titles = []\n",
        "  bodies = []\n",
        "  for sentence in title_sentences:\n",
        "        sentence = re.sub('<[^<]+?>', '', sentence)\n",
        "        words = nltk.word_tokenize(sentence)\n",
        "        titles.append(words)\n",
        "  for sentence in body_sentences:\n",
        "        sentence = re.sub('<[^<]+?>', '', sentence)\n",
        "        words = nltk.word_tokenize(sentence)\n",
        "        bodies.append(words)\n",
        "  lst_training_sentences.extend(titles)\n",
        "  lst_training_sentences.extend(bodies)\n",
        "  \n",
        "  lst_answers = question.answers\n",
        "\n",
        "  if lst_answers is not None:\n",
        "    for answer in lst_answers:\n",
        "      answer_body = answer.body\n",
        "      answer_sentences = nltk.sent_tokenize(answer_body)\n",
        "      answers = []\n",
        "\n",
        "      for sentence in answer_sentences:\n",
        "        sentence = re.sub('<[^<]+?>', '', sentence)\n",
        "        words = nltk.word_tokenize(sentence)\n",
        "        answers.append(words)\n",
        "      lst_training_sentences.extend(answers)\n",
        "# train your model\n",
        "\n",
        "model = train_model(lst_training_sentences)"
      ],
      "metadata": {
        "id": "M7PNV1oxfBbh"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load our model\n",
        "model = FastText.load(\"myFastText.model\")"
      ],
      "metadata": {
        "id": "VF8m9vrf0xPv"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get embeddings\n",
        "def get_embeddings_title_body(model, dictionary, map):\n",
        "  embeddings = {}\n",
        "  for id in dictionary:\n",
        "    question = map[id]\n",
        "    title = re.sub('<[^<]+?>', '', question.title)\n",
        "    body = re.sub('<[^<]+?>', '', question.body)\n",
        "    title_sentences = nltk.sent_tokenize(title)\n",
        "    body_sentences = nltk.sent_tokenize(body)\n",
        "    title_embeddings = []\n",
        "    body_embeddings = []\n",
        "\n",
        "    for sentence in title_sentences:\n",
        "      title_embeddings.append(get_sentence_embedding(model, sentence))\n",
        "\n",
        "    # Get the average title embedding\n",
        "    average_title_embedding = np.mean(title_embeddings, axis=0)\n",
        "\n",
        "    for sentence in body_sentences:\n",
        "      body_embeddings.append(get_sentence_embedding(model, sentence))\n",
        "\n",
        "    # Get the average body embedding\n",
        "    average_body_embedding = np.mean(body_embeddings, axis=0)\n",
        "\n",
        "    embeddings[id] = [average_title_embedding, average_body_embedding]\n",
        "    #print(id)\n",
        "  return embeddings"
      ],
      "metadata": {
        "id": "xMhxyuA2qiIY"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_embeddings = get_embeddings_title_body(model, post_reader.map_questions, post_reader.map_questions)\n",
        "positive_embeddings = get_embeddings_title_body(model, dic_similar_questions, post_reader.map_questions)"
      ],
      "metadata": {
        "id": "dV2_SKvDuSRA"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cos_sim(v1, v2):\n",
        "  return 1 - spatial.distance.cosine(v1, v2)"
      ],
      "metadata": {
        "id": "rJa8PCPf144_"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def update_most_similar(potential_max, current_max, potential_id, current_id):\n",
        "  if potential_max > current_max:\n",
        "    return potential_max, potential_id\n",
        "  else:\n",
        "    return current_max, current_id"
      ],
      "metadata": {
        "id": "--ZFSkRLFjtZ"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "def get_similarities(model, all_embeddings, positive_embeddings, dic_similar_questions):\n",
        "  # This dictionary will have the test question id as the key\n",
        "  # and the most similar question id as the value\n",
        "  dictionary_result_title = {}\n",
        "  dictionary_result_body = {}\n",
        "\n",
        "  # finding Similar questions using fastText model\n",
        "  for test_question_id in dic_similar_questions:\n",
        "      max_title_sim = -1\n",
        "      max_body_sim = -1\n",
        "      most_sim_title_id = -1\n",
        "      most_sim_body_id = -1\n",
        "\n",
        "      for embedding in all_embeddings:\n",
        "        # we are not comparing a question with itself\n",
        "        if embedding == test_question_id:\n",
        "          continue\n",
        "\n",
        "        # Get similarities for titles only\n",
        "        #title_similarity = 1 - spatial.distance.cosine(all_positive_embeddings[test_question_id][0], all_embeddings[question_id][0])\n",
        "\n",
        "        title_v1 = positive_embeddings[test_question_id][0]\n",
        "        title_v2 = all_embeddings[embedding][0]\n",
        "        #print(title_v1)\n",
        "        #print(title_v2)\n",
        "\n",
        "        title_sim = cos_sim(title_v1, title_v2)\n",
        "        #print(title_sim)\n",
        "        #time.sleep(20)\n",
        "        #print(test_question_id, \":\", title_sim)\n",
        "\n",
        "        # Get similarities for bodies only\n",
        "        body_v1 = positive_embeddings[test_question_id][1]\n",
        "        body_v2 = all_embeddings[embedding][1]\n",
        "\n",
        "        body_sim = cos_sim(body_v1, body_v2)\n",
        "        #print(body_sim)\n",
        "        #time.sleep(20)\n",
        "\n",
        "        max_title_sim, most_sim_title_id = update_most_similar(title_sim, \n",
        "                                                               max_title_sim, \n",
        "                                                               embedding, \n",
        "                                                               most_sim_title_id)\n",
        "        max_body_sim, most_sim_body_id = update_most_similar(body_sim,\n",
        "                                                              max_body_sim, \n",
        "                                                              embedding, \n",
        "                                                              most_sim_body_id)\n",
        "        #print(max_title_sim) \n",
        "        #print(max_body_sim) \n",
        "        #print(most_sim_title_id)\n",
        "        #print(most_sim_body_id)\n",
        "        #time.sleep(3)\n",
        "      dictionary_result_title[test_question_id] = most_sim_title_id\n",
        "      dictionary_result_body[test_question_id] = most_sim_body_id\n",
        "\n",
        "  return dictionary_result_title, dictionary_result_body"
      ],
      "metadata": {
        "id": "VrJUs4RMuoJo"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dic_result_title, dic_result_body = get_similarities(model, all_embeddings, positive_embeddings, dic_similar_questions)"
      ],
      "metadata": {
        "id": "pmfUAG-nHvcp"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def p_at_1(dic, dic_similar_questions, type):\n",
        "  sum = 0\n",
        "\n",
        "  for id in dic:\n",
        "    if dic[id] in dic_similar_questions:\n",
        "      sum += 1\n",
        "  \n",
        "  p_at_1_avg = sum / len(dic)\n",
        "\n",
        "  print(sum, \"matches for question \", type)\n",
        "  print(\"Average (\", type, \"): \", p_at_1_avg)"
      ],
      "metadata": {
        "id": "C8gNydYfICb-"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p_at_1(dic_result_title, dic_similar_questions, \"titles\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6JQEqVNJKLaS",
        "outputId": "2cd4a238-50f9-4092-e28c-82c00ee9e34f"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4 matches for question  titles\n",
            "Average ( titles ):  0.014184397163120567\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "p_at_1(dic_result_body, dic_similar_questions, \"bodies\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "04VwB1VEKUWA",
        "outputId": "dd094985-b0b1-464f-ac4e-e22877a8f72d"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3 matches for question  bodies\n",
            "Average ( bodies ):  0.010638297872340425\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 2"
      ],
      "metadata": {
        "id": "o9DPHmwfbSQZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "class FeedforwardNeuralNetModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim_1, hidden_dim_2, hidden_dim_3, output_dim):\n",
        "        super(FeedforwardNeuralNetModel, self).__init__()\n",
        "        # Define layers\n",
        "\n",
        "        self.layer_1 = nn.Linear(input_dim, hidden_dim_1)\n",
        "\n",
        "        self.relu_1 = nn.ReLU()\n",
        "\n",
        "        self.layer_2 = nn.Linear(hidden_dim_1, hidden_dim_2)\n",
        "\n",
        "        self.relu_2 = nn.ReLU()\n",
        "\n",
        "        self.layer_3 = nn.Linear(hidden_dim_2, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Your network forward pass\n",
        "\n",
        "        # modify this line\n",
        "\n",
        "        out = self.layer_1(x)\n",
        "        out = self.relu_1(out)\n",
        "\n",
        "        out = self.layer_2(out)\n",
        "        out = self.relu_2(out)\n",
        "\n",
        "        out = self.layer_3(out)\n",
        "\n",
        "        return torch.sigmoid(out)"
      ],
      "metadata": {
        "id": "W6zvWq8abUod"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_train_val_losses(train_losses, val_losses):\n",
        "    # Plot the training and validation losses\n",
        "    plt.plot(train_losses, label='Train')\n",
        "    plt.plot(val_losses, label='Validation')\n",
        "    plt.title('Training and Validation Losses')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "MItmED6qci2N"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# here goes your parameters\n",
        "# sample code to define your model \n",
        "#model = FeedforwardNeuralNetModel(input_dim, hidden_dim_1, out_dim)\n",
        "#model.to(devide)\n",
        "input_dim = 100\n",
        "hidden_dim_1 = 16\n",
        "hidden_dim_2 = 32\n",
        "hidden_dim_3 = 32\n",
        "output_dim = 3\n",
        "num_epochs = 10000\n",
        "model = FeedforwardNeuralNetModel(input_dim, hidden_dim_1, hidden_dim_2, \n",
        "                                  hidden_dim_3, output_dim)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1e-3)\n",
        "model.to(device)\n",
        "criterion = criterion.to(device)\n",
        "\n",
        "\n",
        "def calculate_accuracy(y_true, y_pred):\n",
        "  # this method will be used to calculate the accuracy of your model\n",
        "    correct = (y_true.argmax (dim = 1) == y_pred.argmax (dim = 1)).float()\n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc\n",
        "def training(tfidfX_train, Y_train, tfidfX_val, Y_val):\n",
        "  # this method will be used for training your model\n",
        "  # inputs are the training and validation sets\n",
        "  # You can define batch size of your choice\n",
        "  batch_size = 2000\n",
        "  #print(type(tfidfX_train))\n",
        "  X_train_mini_batches = torch.split(tfidfX_train, batch_size)\n",
        "  Y_train_mini_batches = torch.split(Y_train, batch_size)\n",
        "  train_losses = []\n",
        "  train_accuracies = []\n",
        "  val_losses = []\n",
        "  val_accuracies = []\n",
        "  best_accuracy = 0\n",
        "  for epoch in tqdm(range(num_epochs)):\n",
        "      epoch_loss = 0\n",
        "      epoch_accuracy = 0\n",
        "      validation_loss = 0\n",
        "      val_accuracy = 0\n",
        "      for X_train_mini_batch, Y_train_mini_batch in zip(X_train_mini_batches, Y_train_mini_batches):\n",
        "          X_train_mini_batch = X_train_mini_batch.to(device)\n",
        "          Y_train_mini_batch = Y_train_mini_batch.to(device)\n",
        "          # Continue code here to train the network\n",
        "          # here check your validation set\n",
        "          # you have to save the model with the best loss or maybe accuracy?\n",
        "          train_prediction = model.forward(X_train_mini_batch.float())\n",
        "          train_prediction = torch.squeeze(train_prediction)\n",
        "          train_loss = criterion(train_prediction, Y_train_mini_batch.float())\n",
        "          optimizer.zero_grad()\n",
        "          train_loss.backward()\n",
        "          optimizer.step()\n",
        "          epoch_loss += train_loss.item()\n",
        "          epoch_accuracy += calculate_accuracy(Y_train_mini_batch, train_prediction)\n",
        "\n",
        "      tfidfX_val = tfidfX_val.to(device)\n",
        "      Y_val = Y_val.to(device)\n",
        "      val_prediction = model.forward(tfidfX_val.float())\n",
        "      val_prediction = torch.squeeze(val_prediction)\n",
        "      val_loss = criterion(val_prediction, Y_val.float())\n",
        "      validation_loss = val_loss.item()\n",
        "      val_accuracy = calculate_accuracy(Y_val, val_prediction)\n",
        "      if val_accuracy > best_accuracy:\n",
        "        torch.save(model.state_dict(), \"best_model_state.bin\")\n",
        "        best_accuracy = val_accuracy\n",
        "      epoch_loss /= len(X_train_mini_batches)\n",
        "      epoch_accuracy /= len(X_train_mini_batches)\n",
        "      val_losses.append(validation_loss)\n",
        "      train_losses.append(epoch_loss)\n",
        "      train_accuracies.append(epoch_accuracy)\n",
        "\n",
        "      #plot_train_val_losses(train_losses, val_losses)\n",
        "\n",
        "      val_accuracies.append(val_accuracy)"
      ],
      "metadata": {
        "id": "u7oxWG81clhg"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FP9L59NDfEhA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model.load_state_dict(torch.load('best_model_state.bin'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        },
        "id": "m8uqUdAGekWQ",
        "outputId": "62d9759d-9d27-4a91-e162-49a748833a5c"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-118-6c0fc6fce3db>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'best_model_state.bin'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 791\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    792\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'best_model_state.bin'"
          ]
        }
      ]
    }
  ]
}